[fullyConnectedLayer]
input_size = 784
output_size = 128
activation_func = relu

[fullyConnectedLayer]
input_size = 128
output_size = 1
activation_func = sigmoid